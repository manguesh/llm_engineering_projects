{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise - explaination for a technical question\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "\n",
    "# If you get an error running this cell, then please head over to the troubleshooting notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a771894-cccc-4e9b-a905-427d582c1930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â � \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling dde5aa3fc5ff... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 2.0 GB                         \n",
      "pulling 966de95ca8a6... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 1.4 KB                         \n",
      "pulling fcc5a6bec9da... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 7.7 KB                         \n",
      "pulling a70ff7e570d9... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 6.0 KB                         \n",
      "pulling 56bb8bd477a5... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�   96 B                         \n",
      "pulling 34bb5ab01051... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�  561 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93629ced-16fd-4169-8d28-c18b8b9a7a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Let's break down the code snippet you provided:\n",
      "\n",
      "```python\n",
      "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
      "```\n",
      "\n",
      "### Key Components:\n",
      "\n",
      "1. **`yield from`**: This is a special syntax in Python used to delegate part of a generator's operations to another generator. It allows you to yield all values from an iterable (like a list, a set, etc.) one by one, as if they were being yielded directly from the outer generator. Essentially, it helps in creating a generator that can yield results from another source.\n",
      "\n",
      "2. **Set Comprehension**: The part inside the curly braces `{}` is a set comprehension. Set comprehensions are a concise way to create sets in Python. The general structure is `{expression for item in iterable if condition}`. In your code, this means you're creating a set of authors from the `books` list.\n",
      "\n",
      "3. **`book.get(\"author\")`**: This method attempts to retrieve the value associated with the key `\"author\"` from the dictionary `book`. If the key does not exist, it will return `None` (or a specified default if given, but that is not the case here).\n",
      "\n",
      "4. **`for book in books`**: This is a loop iterating over an iterable called `books`, which is presumably a list of dictionaries, where each dictionary contains information about a book, including an author.\n",
      "\n",
      "5. **`if book.get(\"author\")`**: This condition filters the books, only including those that have a valid author (i.e., the value is not `None` or an empty string). \n",
      "\n",
      "### What the Code Does:\n",
      "\n",
      "1. It iterates over a list of `books`, where each `book` is a dictionary.\n",
      "2. For each `book`, it attempts to get the value of the key `\"author\"`.\n",
      "3. If the `author` exists (the value is not `None` or empty), it adds it to a set of authors. Using a set ensures that each author is unique; if the same author appears in multiple books, they will only be stored once.\n",
      "4. `yield from` then yields each author in this set one by one when the outer generator is iterated over.\n",
      "\n",
      "### Why Use This Code?\n",
      "\n",
      "- **Uniqueness**: By using a set, it guarantees that the list of authors will be unique without requiring additional logic to check for duplicates.\n",
      "- **Simplicity and Readability**: The set comprehension makes the code concise and easy to read compared to using a loop and a conditional to build the set.\n",
      "- **Generators**: By utilizing `yield from`, it allows for lazy evaluation, meaning that authors are generated on-the-fly when requested, rather than all at once, which can save memory and improve performance.\n",
      "\n",
      "### Summary:\n",
      "\n",
      "This code snippet creates a generator that yields unique authors from a list of books, ensuring that each author is represented only once. It efficiently handles the retrieval and filtering of authors using set comprehension and the `yield from` statement.\n"
     ]
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "# Step 1: Create your prompts\n",
    "\n",
    "system_prompt = \"You are an expert who explains every technical question.You are a great teacher who simplifies the concepts and makes it easy to undertand\"\n",
    "\n",
    "# Step 2: Make the messages list\n",
    "\n",
    "def questions_to_explain(question):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "# Step 3: Call OpenAI\n",
    "def explain_technical_question_with_gpt(question):\n",
    "    response = openai.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = questions_to_explain(question)\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "    \n",
    "response = explain_technical_question_with_gpt(question)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "375f8b45-2767-4448-863a-64a92533d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "351a3d53-b218-4768-a088-fc1fb9d7c01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break down this code step by step.\n",
      "\n",
      "This is a Python snippet that utilizes generators. A generator is a special type of function that can be used to generate a sequence of results, instead of computing them all at once and returning them in a list or other data structure.\n",
      "\n",
      "In this specific case, we have the following components:\n",
      "\n",
      "1. `yield from`: This keyword is used to delegate to another generator (or iterable) and yield from it when the current function is suspended.\n",
      "2. `{book.get(\"author\") for book in books if book.get(\"author\")}`: This is a generator expression, which is a compact way to create an iterable.\n",
      "\n",
      "Here's what happens inside this generator:\n",
      "\n",
      "1. `for book in books`: We iterate over each item (`book`) in the `books` collection.\n",
      "2. `if book.get(\"author\")`: For each item, we check if it has a key called `\"author\"`. If it does, then...\n",
      "3. `book.get(\"author\")`: We yield (produce) the value of that key. This is where the magic happens: when we're iterating over our collections using generators, it's not loading everything into memory at once.\n",
      "\n",
      "Now, let's put it all together:\n",
      "\n",
      "```python\n",
      "# Suppose books is a list or collection of dictionaries.\n",
      "books = [\n",
      "    {\"author\": \"John\"},\n",
      "    {\"title\": \"Book 1\", \"author\": \"Jane\"},\n",
      "    # Add more...\n",
      "]\n",
      "\n",
      "# Now, run the code we discussed earlier:\n",
      "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
      "```\n",
      "\n",
      "**Why does it matter?**\n",
      "\n",
      "In traditional Python iteration, when you use `for` to iterate over a collection of objects, each object is loaded into memory at once. This can be inefficient and even lead to problems like:\n",
      "\n",
      "- Running out of memory (e.g., for huge datasets)\n",
      "- Slower performance due to unnecessary data loading\n",
      "\n",
      "With this code, we're using generators, which allow us to lazily compute and yield values one by one. This has many benefits, such as:\n",
      "\n",
      "- Less memory usage\n",
      "- Potential speed improvements\n",
      "- Better control when processing big datasets\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Call OpenAI\n",
    "\n",
    "def explain_technical_question_with_ollama(question):\n",
    "    from openai import OpenAI\n",
    "    ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "    response = ollama_via_openai.chat.completions.create(\n",
    "        model=\"llama3.2\",\n",
    "        messages=questions_to_explain(question)\n",
    "    )\n",
    "    return response.choices[0].message.content    \n",
    "    \n",
    "response = explain_technical_question_with_ollama(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
