{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe37963-1af6-44fc-a841-8e462443f5e6",
   "metadata": {},
   "source": [
    "## Personal Laptop Assistant\n",
    "\n",
    "### A question answering agent that is an expert knowledge worker\n",
    "### To be used by persons who have saved lot of documents on their\n",
    "### laptops and find it difficult to fetch information from them \n",
    "### with minimum effort and time\n",
    "### The agent needs to be accurate and the solution should be low cost.\n",
    "\n",
    "This project will use RAG (Retrieval Augmented Generation) to ensure our question/answering assistant has high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain, plotly and Chroma\n",
    "\n",
    "#from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "#from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "#from langchain_chroma import Chroma\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a661105-dedd-42eb-93c7-63019cf17b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from langchain.document_loaders import (\n",
    "    DirectoryLoader,\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ae240e-8f99-486c-a67b-188c28218225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"claude-3-haiku-20240307\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6235877-e598-4ba5-9b9e-fafca635291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORTED_EXTENSIONS = {\n",
    "    '.txt': TextLoader,\n",
    "    '.md': TextLoader,\n",
    "    '.pdf': PyPDFLoader,\n",
    "    '.docx': UnstructuredWordDocumentLoader,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e241d479-a420-4e49-bace-be175afeb567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_folder(folder_path):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            ext = os.path.splitext(file)[1].lower()\n",
    "            loader_class = SUPPORTED_EXTENSIONS.get(ext)\n",
    "\n",
    "            if not loader_class:\n",
    "                print(f\"Skipping unsupported file: {file_path}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                print(f\"Loading: {file_path}\")\n",
    "                loader = loader_class(file_path)\n",
    "                docs = loader.load()\n",
    "                documents.extend(docs)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {file_path}: {e}\")\n",
    "                traceback.print_exc()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0beb1f19-77ea-4171-80a9-f2dbc20c27a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Scanning folder: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\company\\about.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\company\\careers.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\company\\overview.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\company\\.ipynb_checkpoints\\careers-checkpoint.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\company\\.ipynb_checkpoints\\overview-checkpoint.md\n",
      "Skipping unsupported file: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\Contract with Apex Reinsurance for Rellm.md\n",
      "Failed to load C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\Contract with Apex Reinsurance for Rellm.md: Error loading C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\Contract with Apex Reinsurance for Rellm.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\Contract with Belvedere Insurance for Markellm.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\Contract with BrightWay Solutions for Markellm.md\n",
      "Failed to load C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\Contract with BrightWay Solutions for Markellm.md: Error loading C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\Contract with BrightWay Solutions for Markellm.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\Contract with EverGuard Insurance for Rellm.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\Contract with GreenField Holdings for Markellm.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\Contract with Greenstone Insurance for Homellm.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\Contract with GreenValley Insurance for Homellm.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\Contract with Pinnacle Insurance Co. for Homellm.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\Contract with Roadway Insurance Inc. for Carllm.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\Contract with Stellar Insurance Co. for Rellm.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\Contract with TechDrive Insurance for Carllm.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\Contract with Velocity Auto Solutions for Carllm.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\.ipynb_checkpoints\\Contract with Belvedere Insurance for Markellm-checkpoint.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\.ipynb_checkpoints\\Contract with GreenField Holdings for Markellm-checkpoint.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\important-documents\\bishops.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py\", line 43, in lazy_load\n",
      "    text = f.read()\n",
      "           ^^^^^^^^\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\encodings\\cp1252.py\", line 23, in decode\n",
      "    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 156: character maps to <undefined>\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mangu\\AppData\\Local\\Temp\\ipykernel_37152\\754545630.py\", line 16, in load_documents_from_folder\n",
      "    docs = loader.load()\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py\", line 32, in load\n",
      "    return list(self.lazy_load())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py\", line 56, in lazy_load\n",
      "    raise RuntimeError(f\"Error loading {self.file_path}\") from e\n",
      "RuntimeError: Error loading C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\Contract with Apex Reinsurance for Rellm.md\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py\", line 43, in lazy_load\n",
      "    text = f.read()\n",
      "           ^^^^^^^^\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\encodings\\cp1252.py\", line 23, in decode\n",
      "    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 163: character maps to <undefined>\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mangu\\AppData\\Local\\Temp\\ipykernel_37152\\754545630.py\", line 16, in load_documents_from_folder\n",
      "    docs = loader.load()\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py\", line 32, in load\n",
      "    return list(self.lazy_load())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py\", line 56, in lazy_load\n",
      "    raise RuntimeError(f\"Error loading {self.file_path}\") from e\n",
      "RuntimeError: Error loading C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\Contract with BrightWay Solutions for Markellm.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\contracts\\important-documents\\Trupti Manguesh Borker.pdf\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Alex Chen.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Alex Harper.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Alex Thomson.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Avery Lancaster.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\eBook-Scaling-RAG-Systems-from-POC-to-Production-‚Äì-2025.pdf\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Emily Carter.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Emily Tran.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Jordan Blake.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Jordan K. Bishop.md\n",
      "Failed to load C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Jordan K. Bishop.md: Error loading C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Jordan K. Bishop.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\MANGUESH DATTA BORKER_01-04-2025.docx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py\", line 43, in lazy_load\n",
      "    text = f.read()\n",
      "           ^^^^^^^^\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\encodings\\cp1252.py\", line 23, in decode\n",
      "    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 1305: character maps to <undefined>\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mangu\\AppData\\Local\\Temp\\ipykernel_37152\\754545630.py\", line 16, in load_documents_from_folder\n",
      "    docs = loader.load()\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py\", line 32, in load\n",
      "    return list(self.lazy_load())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py\", line 56, in lazy_load\n",
      "    raise RuntimeError(f\"Error loading {self.file_path}\") from e\n",
      "RuntimeError: Error loading C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Jordan K. Bishop.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Maxine Thompson.md\n",
      "Failed to load C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Maxine Thompson.md: Error loading C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Maxine Thompson.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Oliver Spencer.md\n",
      "Failed to load C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Oliver Spencer.md: Error loading C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Oliver Spencer.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Samantha Greene.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Samuel Trenton.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\.ipynb_checkpoints\\Jordan K. Bishop-checkpoint.md\n",
      "Failed to load C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\.ipynb_checkpoints\\Jordan K. Bishop-checkpoint.md: Error loading C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\.ipynb_checkpoints\\Jordan K. Bishop-checkpoint.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\products\\Carllm.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\products\\Homellm.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\products\\Markellm.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\products\\Rellm.md\n",
      "Loading: C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\products\\.ipynb_checkpoints\\Homellm-checkpoint.md\n",
      "\n",
      "‚úÇÔ∏è Splitting documents into chunks...\n",
      "\n",
      "üß† Creating embeddings...\n",
      "\n",
      "üíæ Storing embeddings in Chroma DB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py\", line 43, in lazy_load\n",
      "    text = f.read()\n",
      "           ^^^^^^^^\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\encodings\\cp1252.py\", line 23, in decode\n",
      "    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 3419: character maps to <undefined>\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mangu\\AppData\\Local\\Temp\\ipykernel_37152\\754545630.py\", line 16, in load_documents_from_folder\n",
      "    docs = loader.load()\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py\", line 32, in load\n",
      "    return list(self.lazy_load())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py\", line 56, in lazy_load\n",
      "    raise RuntimeError(f\"Error loading {self.file_path}\") from e\n",
      "RuntimeError: Error loading C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Maxine Thompson.md\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py\", line 43, in lazy_load\n",
      "    text = f.read()\n",
      "           ^^^^^^^^\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\encodings\\cp1252.py\", line 23, in decode\n",
      "    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 2267: character maps to <undefined>\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mangu\\AppData\\Local\\Temp\\ipykernel_37152\\754545630.py\", line 16, in load_documents_from_folder\n",
      "    docs = loader.load()\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py\", line 32, in load\n",
      "    return list(self.lazy_load())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py\", line 56, in lazy_load\n",
      "    raise RuntimeError(f\"Error loading {self.file_path}\") from e\n",
      "RuntimeError: Error loading C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\Oliver Spencer.md\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py\", line 43, in lazy_load\n",
      "    text = f.read()\n",
      "           ^^^^^^^^\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\encodings\\cp1252.py\", line 23, in decode\n",
      "    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 1305: character maps to <undefined>\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mangu\\AppData\\Local\\Temp\\ipykernel_37152\\754545630.py\", line 16, in load_documents_from_folder\n",
      "    docs = loader.load()\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py\", line 32, in load\n",
      "    return list(self.lazy_load())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mangu\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py\", line 56, in lazy_load\n",
      "    raise RuntimeError(f\"Error loading {self.file_path}\") from e\n",
      "RuntimeError: Error loading C:\\Users\\mangu\\Documents\\Project\\llm_engineering\\week5\\knowledge-base\\employees\\.ipynb_checkpoints\\Jordan K. Bishop-checkpoint.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Embeddings successfully saved to Chroma at: chroma_db\n",
      "Vectorstore created with 3522 documents\n",
      "There are 3,522 vectors with 1,536 dimensions in the vector store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mangu\\AppData\\Local\\Temp\\ipykernel_37152\\3552557825.py:24: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"C:\\\\Users\\\\mangu\\\\Documents\\\\Project\\\\llm_engineering\\\\week5\\\\knowledge-base\"  # <- Replace this with your folder path\n",
    "    try:\n",
    "        print(f\"\\nüìÅ Scanning folder: {folder_path}\")\n",
    "        documents = load_documents_from_folder(folder_path)\n",
    "\n",
    "        if not documents:\n",
    "            print(\"No documents found. Exiting.\")\n",
    "            exit\n",
    "\n",
    "        print(\"\\n‚úÇÔ∏è Splitting documents into chunks...\")\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        split_docs = splitter.split_documents(documents)\n",
    "\n",
    "        print(\"\\nüß† Creating embeddings...\")\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "\n",
    "        # Delete if already exists\n",
    "        if os.path.exists(db_name):\n",
    "            Chroma(persist_directory=\"chroma_db\", embedding_function=embeddings).delete_collection()\n",
    "    \n",
    "        print(\"\\nüíæ Storing embeddings in Chroma DB...\")\n",
    "        vectordb = Chroma.from_documents(documents, embedding=embeddings, persist_directory=\"chroma_db\")\n",
    "        vectordb.persist()\n",
    "\n",
    "        print(f\"\\n‚úÖ Embeddings successfully saved to Chroma at: chroma_db\")\n",
    "        print(f\"Vectorstore created with {vectordb._collection.count()} documents\")\n",
    "\n",
    "        # Let's investigate the vectors\n",
    "\n",
    "        collection = vectordb._collection\n",
    "        count = collection.count()\n",
    "        \n",
    "        sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "        dimensions = len(sample_embedding)\n",
    "        print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error during embedding creation: {e}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2136153b-d2f6-4c58-a0e3-78c3a932cf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mangu\\AppData\\Local\\Temp\\ipykernel_37152\\927671916.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Create a new Chat with Anthropic (Claude Haiku)\n",
    "llm = ChatAnthropic(temperature=0.7, model=\"claude-3-haiku-20240307\")\n",
    "\n",
    "# Set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# The retriever is an abstraction over the VectorStore that will be used during RAG; k is how many chunks to use\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 50})\n",
    "\n",
    "# Putting it together: set up the conversation chain with Claude Haiku, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c2bfa3c-810b-441b-90d1-31533f14b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    print(\"\\nAnswer:\", result[\"answer\"])\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c736f33b-941e-4853-8eaf-2003bd988b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7890\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7890/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Hello! How can I assist you today?\n",
      "\n",
      "Answer: Apologies, it seems there was some confusion. The previous conversation did not mention anyone named Trupti, so I do not have any specific information about this person. Without more context about who Trupti is and what you would like to know, I'm unable to provide a meaningful response. Could you please provide some additional details so I can better understand your query? I'm happy to try and assist further once I have more information.\n",
      "\n",
      "Answer: Based on the information provided, Manguesh Datta Borker appears to be an experienced strategic technology leader with over 2 decades of experience in managing complex technical environments for billion-dollar industry leaders. Some key points about him:\n",
      "\n",
      "- He has a robust, progressive leadership experience in strategic technology leadership, digital transformation, IT strategy, and business innovation.\n",
      "\n",
      "- He is described as a distinguished technology luminary and enterprising entrepreneur with a wealth of experience spanning Fortune 500 corporations and expanding startups.\n",
      "\n",
      "- He has demonstrated expertise in pioneering innovative solutions, crafting strategic roadmaps, and executing with fiscal acumen to achieve transformative outcomes.\n",
      "\n",
      "- His profile summary highlights his experience as a visionary business executive with both entrepreneurial and corporate experience, founding and maturing technology startups, driving customer-centric technology products, and building strategic partnerships.\n",
      "\n",
      "- He is portrayed as a versatile full-stack tech leader with proven expertise spanning backend, frontend, databases, DevOps, cloud, and generative AI/LLM technologies.\n",
      "\n",
      "So in summary, Manguesh Borker seems to be an accomplished technology leader and entrepreneur with a strong track record of driving digital transformation and business innovation across large enterprises and startups.\n",
      "\n",
      "Answer: Unfortunately I do not have any specific information about Trupti Borker that I can provide. The previous conversation did not mention this individual, so I do not have any details about them that I can share. Without additional context about who Trupti Borker is and what you would like to know, I do not have enough information to answer a question about this person. If you are able to provide more details, I would be happy to try and assist further. But based on the information given so far, I do not have any relevant details about Trupti Borker that I can respond with. Please let me know if you can give me more context about this person or topic.\n"
     ]
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644753e7-17f3-4999-a37a-b6aebf1e4579",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Try applying this to your own folder of data, so that you create a personal knowledge worker, an expert on your own information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b4745a-0a6c-4544-b78b-c827cfec1fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
